{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "whole-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-memorabilia",
   "metadata": {},
   "source": [
    "### We will learn how to put it all together later\n",
    "\n",
    "for now we will dive into separate sections  and focus on them individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-domain",
   "metadata": {},
   "source": [
    "## Note: - \n",
    "\n",
    "We will learn many methods(ways/options) to transform data, clean data, fill in missing value, \n",
    "split data into train and test set, and so on.....\n",
    "\n",
    "Here, we will use one method (the most useful ones) among many available to us for each process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-limit",
   "metadata": {},
   "source": [
    "# 1) Getting the data ready\n",
    "\n",
    "Main steps to take: - \n",
    "\n",
    "1. Splitting the data into features (usually X) and labels (usually y) \n",
    "2. Filling data (also called imputation) or disregarding missing values\n",
    "3. Converting non-numerical values to numerical values (also call feature encoding)\n",
    "4. Making sure all of your numerical data is on the same scale - Feature scaling by either Normalization/Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-found",
   "metadata": {},
   "source": [
    "## 1.1) Splitting the data into training, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-playback",
   "metadata": {},
   "source": [
    "## 1.2) and, Dealing with missing values in data\n",
    "\n",
    "1. Fill them with some value (also known as imputation) - using fillna(), OR\n",
    "2. Remove the rows with missing data - using dropna()\n",
    "\n",
    "Two methods --> both are okay\n",
    "\n",
    "Missing values are labeled as NaN in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "worthy-retention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14043.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make Colour  Odometer (KM)  Doors    Price\n",
       "0   Honda  White        35431.0    4.0  15323.0\n",
       "1     BMW   Blue       192714.0    5.0  19943.0\n",
       "2   Honda  White        84714.0    4.0  28343.0\n",
       "3  Toyota  White       154365.0    4.0  13434.0\n",
       "4  Nissan   Blue       181577.0    3.0  14043.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_df = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "car_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-forest",
   "metadata": {},
   "source": [
    "In dataframe - axis = 1 --> column axis, axis = 0 -> row axis\n",
    "\n",
    "Imp principle: - In ML, never evaluate or test ur models on data that it has learned from. \n",
    "Thats why, we split data.\n",
    "\n",
    "* Build a ML model that will train on the training data, and predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "toxic-future",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make              object\n",
       "Colour            object\n",
       "Odometer (KM)    float64\n",
       "Doors            float64\n",
       "Price            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatype of the columns\n",
    "car_df.dtypes\n",
    "\n",
    "# If Price column was object, convert it from\n",
    "# 1) object --> string, then  2) string --> int \n",
    "# Check matplotlib notes in my notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "assured-attention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many data is present in the dataframe\n",
    "len(car_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "auburn-halloween",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             49\n",
       "Colour           50\n",
       "Odometer (KM)    50\n",
       "Doors            50\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check How many missing values are there in each column\n",
    "car_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-asthma",
   "metadata": {},
   "source": [
    "### IMP POINST TO REMEMBER\n",
    "\n",
    "* Split your data first (into train/validation/test) set, always keep your training, validationtion and test data separate\n",
    "* Fill/transform the training set, validation set and test sets separately\n",
    "* Don't use data from the future (test set) to fill data from the past (training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cubic-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with no labels - i.e. Remove rows with missing data\n",
    "\n",
    "# e.g Lets drop rows with missing price values\n",
    "car_df.dropna(subset=[\"Price\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "assumed-fountain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             47\n",
       "Colour           46\n",
       "Odometer (KM)    48\n",
       "Doors            47\n",
       "Price             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "car_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-valuable",
   "metadata": {},
   "source": [
    "### Split data into training, validation and test sets\n",
    "\n",
    "the data is split into train, validation and test sets before filling any missing values or transformations that will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "popular-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X & Y (datafram variables)\n",
    "\n",
    "# e.g X --> all columns except Price\n",
    "# e.g y --> only Price column\n",
    "\n",
    "X = car_df.drop(\"Price\",axis=1)\n",
    "y = car_df[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "golden-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state -> Controls the shuffling applied to the data before applying the split\n",
    "# test_size -> proportion of the dataset to include in the test split.\n",
    "\n",
    "# We will get 4 different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "macro-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation and test set\n",
    "\n",
    "# e.g. predefined proportions such as (75, 15, 10)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1 - train_ratio)\n",
    "# 1-0.75 = 0.25=25%, so training set is 0.75 =75% of entire dataset\n",
    "\n",
    "# Remaining = 25% = 0.25 in test dataset\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "# 0.10/(0.10+0.15) = 0.4 x 25% = 10%\n",
    "# test set is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "portuguese-dylan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       Make Colour  Odometer (KM)  Doors\n",
       " 941  Toyota    Red       166046.0    4.0\n",
       " 482  Nissan  White        51004.0    4.0\n",
       " 28    Honda  White        56687.0    4.0\n",
       " 227     BMW   Blue        79301.0    5.0\n",
       " 203  Toyota   Blue        99761.0    4.0\n",
       " ..      ...    ...            ...    ...\n",
       " 202   Honda   Blue        84719.0    4.0\n",
       " 737  Toyota   Blue       223875.0    4.0\n",
       " 109   Honda   Blue       219217.0    4.0\n",
       " 331  Toyota  White       112292.0    4.0\n",
       " 425  Toyota   Blue        42480.0    4.0\n",
       " \n",
       " [712 rows x 4 columns],\n",
       "        Make Colour  Odometer (KM)  Doors\n",
       " 810  Nissan  Green       229929.0    4.0\n",
       " 759  Nissan    Red       113987.0    4.0\n",
       " 184  Nissan  Green        32754.0    4.0\n",
       " 838  Nissan  Green       235589.0    4.0\n",
       " 597  Nissan  White        20315.0    4.0\n",
       " ..      ...    ...            ...    ...\n",
       " 285  Toyota  Green        44436.0    4.0\n",
       " 509  Nissan  White        26634.0    4.0\n",
       " 962   Honda   Blue        96308.0    4.0\n",
       " 972   Honda  White            NaN    4.0\n",
       " 442  Nissan  Black        59335.0    4.0\n",
       " \n",
       " [142 rows x 4 columns],\n",
       "        Make Colour  Odometer (KM)  Doors\n",
       " 284   Honda    Red       238609.0    4.0\n",
       " 848  Toyota   Blue        48684.0    4.0\n",
       " 994     BMW   Blue       163322.0    3.0\n",
       " 206   Honda  White       211862.0    4.0\n",
       " 786     BMW  Green       152243.0    5.0\n",
       " ..      ...    ...            ...    ...\n",
       " 548  Toyota   Blue        89641.0    4.0\n",
       " 441  Nissan  Green        90446.0    4.0\n",
       " 544   Honda  Black        27788.0    4.0\n",
       " 292  Toyota    Red       197616.0    4.0\n",
       " 267   Honda   Blue       136392.0    4.0\n",
       " \n",
       " [96 rows x 4 columns])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "recent-acquisition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 142, 96)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "protective-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in train, validation and test set - How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "essential-invention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             39\n",
       "Colour           34\n",
       "Odometer (KM)    34\n",
       "Doors            42\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "charitable-investigator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make              6\n",
       "Colour            8\n",
       "Odometer (KM)    10\n",
       "Doors             2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "mediterranean-psychology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             2\n",
       "Colour           4\n",
       "Odometer (KM)    4\n",
       "Doors            3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-kennedy",
   "metadata": {},
   "source": [
    "Let's fill the missing values. We'll fill the training, validation and test values separately to ensure training data stays with the training data and test data stays with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-tender",
   "metadata": {},
   "source": [
    "### Option 2: - Filling missing data with Scikit-Learn and transforming categorical data with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-fraud",
   "metadata": {},
   "source": [
    "scikit-learn provides a method called SimpleImputer().\n",
    "\n",
    "SimpleImputer() transforms data by filling missing values with a given strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-petite",
   "metadata": {},
   "source": [
    "Follow order of column\n",
    "\n",
    "* String columns must be filled with strings\n",
    "* Numerical columns can be filled with mean,median,mode or other numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "seasonal-cargo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    569\n",
       "5.0     53\n",
       "3.0     48\n",
       "Name: Doors, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out how many doors are there of each number\n",
    "X_train[\"Doors\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-providence",
   "metadata": {},
   "source": [
    "inplace=True ==> do change to original dataframe (no need to assign to a new df variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-aluminum",
   "metadata": {},
   "source": [
    "Note: We use fit_transform() on the training data and transform() on the testing data. In essence, we learn the patterns in the training set and transform it via imputation (fit, then transform). Then we take those same patterns and fill the test set (transform only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-liabilities",
   "metadata": {},
   "source": [
    "### Methods to know\n",
    "\n",
    "* fit(X,y) - Compute the minimum and maximum to be used for later scaling.\n",
    "\n",
    "* fit_transform(X,y) - Fit to data, then transform it.\n",
    "\n",
    "* transform(X) - Scale features of X according to feature_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "stretch-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with Scikit-Learn\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Fill categorical values with \"missing\" and numerical values with mean\n",
    "# Imputation --> Find the missing values and fill them\n",
    "# Define some imputers\n",
    "\n",
    "# strategy=\"constant\" = go to the categorical columns, if u find a missing value, \n",
    "# constantly fill them with the string missing, or mean or a default value\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\",fill_value=\"missing\")\n",
    "door_imputer = SimpleImputer(strategy=\"constant\",fill_value=4)\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Define columns\n",
    "cat_features = [\"Make\",\"Colour\"] # category column\n",
    "door_features = [\"Doors\"] # a category column\n",
    "num_features = [\"Odometer (KM)\"] # numerical column\n",
    "\n",
    "# Create an imputer (something that fills missing data)\n",
    "# pass in the imputations (all the different transformations to do)\n",
    "\n",
    "# Takes a list and tuples within exists multiple different transformers\n",
    "imputer = ColumnTransformer([\n",
    "    # name, imputer to use, features on which to use the imputer\n",
    "    (\"cat_imputer\",cat_imputer,cat_features),\n",
    "    (\"door_imputer\",door_imputer,door_features),\n",
    "    (\"num_imputer\",num_imputer,num_features)\n",
    "])\n",
    "\n",
    "# Transform data\n",
    "\n",
    "# Fill train,validation and test values separately\n",
    "filled_X_train = imputer.fit_transform(X_train) # fit_transform imputes the missing values from the training set and fills them simultaneously\n",
    "filled_X_val = imputer.fit_transform(X_val)\n",
    "filled_X_test = imputer.transform(X_test) # tranform takes the imputing missing values from the training set and fills the test set with them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "demonstrated-vertex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Toyota', 'Red', 4.0, 166046.0],\n",
       "       ['Nissan', 'White', 4.0, 51004.0],\n",
       "       ['Honda', 'White', 4.0, 56687.0],\n",
       "       ...,\n",
       "       ['Honda', 'Blue', 4.0, 219217.0],\n",
       "       ['Toyota', 'White', 4.0, 112292.0],\n",
       "       ['Toyota', 'Blue', 4.0, 42480.0]], dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check filled X_train\n",
    "filled_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-payment",
   "metadata": {},
   "source": [
    "Now we've filled our missing values, let's check how many are missing from each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "preliminary-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the training, validation and test dataframe to a new one\n",
    "\n",
    "car_sales_filled_train = pd.DataFrame(filled_X_train, \n",
    "                                      columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n",
    "\n",
    "\n",
    "car_sales_filled_val = pd.DataFrame(filled_X_val, \n",
    "                                     columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n",
    "\n",
    "car_sales_filled_test = pd.DataFrame(filled_X_test, \n",
    "                                     columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "premier-livestock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Doors            0\n",
       "Odometer (KM)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing data in training set\n",
    "car_sales_filled_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "disabled-ballet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Doors            0\n",
       "Odometer (KM)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing data in test set\n",
    "car_sales_filled_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "altered-ultimate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Doors            0\n",
       "Odometer (KM)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing data in validation set\n",
    "car_sales_filled_val.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "instant-builder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 142, 96)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many data remaining in the sets\n",
    "len(car_sales_filled_train), len(car_sales_filled_val), len(car_sales_filled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-spiritual",
   "metadata": {},
   "source": [
    "### Now that there are no missing values\n",
    "\n",
    "### Convert the data into numbers (numerical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-nickel",
   "metadata": {},
   "source": [
    "using one hot encoding. Since a ML model cannot deal with strings, only numerical data\n",
    "\n",
    "Again, keeping our training and test data separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fifty-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the categories (Make and Colour) into numbers, as well as Door\n",
    "\n",
    "# remainder=\"passthrough\" --> ignore all other columns other than the ones mentioned above\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Now let's one hot encode the features\n",
    "categorical_features = [\"Make\",\"Colour\",\"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([\n",
    "    (\"one_hot\",one_hot,categorical_features)],\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "# Fill train, validation and test values separately\n",
    "transformed_X_train = transformer.fit_transform(car_sales_filled_train) # fit and transform the training data\n",
    "transformed_X_val = transformer.fit_transform(car_sales_filled_train) # fit and transform the validation data\n",
    "transformed_X_test = transformer.transform(car_sales_filled_test) # transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "respiratory-banks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 1.66046e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 5.10040e+04],\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 5.66870e+04],\n",
       "       ...,\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 2.19217e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 1.12292e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 4.24800e+04]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check transformed and filled X_train\n",
    "transformed_X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "positive-distribution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 1.66046e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 5.10040e+04],\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 5.66870e+04],\n",
       "       ...,\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 2.19217e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 1.12292e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 4.24800e+04]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check transformed and filled X_val\n",
    "transformed_X_val.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eligible-union",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 2.38609e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 4.86840e+04],\n",
       "       [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.63322e+05],\n",
       "       ...,\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 2.77880e+04],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 1.97616e+05],\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 1.36392e+05]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check transformed and filled X_test\n",
    "transformed_X_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-cycle",
   "metadata": {},
   "source": [
    "### Feature Scaling (check it out later if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-fifteen",
   "metadata": {},
   "source": [
    "### In conclusion, \n",
    "\n",
    "The process of filling missing values --> imputation\n",
    "\n",
    "The process of converting non-numerical values to numerical --> feature engineering or feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-regular",
   "metadata": {},
   "source": [
    "### Fit a model\n",
    "\n",
    "Now we've filled and transformed our data, ensuring the training and test sets have been kept separate. Let's fit a model to the training set and evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "sapphire-steering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11996671670109127"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we've transformed X, let's see if we can fit a model\n",
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Make sure to use transformed (filled and one-hot encoded X data)\n",
    "model.fit(transformed_X_train, y_train)\n",
    "model.score(transformed_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-efficiency",
   "metadata": {},
   "source": [
    "Fitting the model on the data involves passing it the data and asking it to figure out the patterns.\n",
    "\n",
    "If there are labels (supervised learning), the model tries to work out the relationship between the data and the labels.\n",
    "\n",
    "If there are no labels (unsupervised learning), the model tries to find patterns and group similar samples together.\n",
    "\n",
    "Use the model to make a prediction\n",
    "The whole point of training a machine learning model is to use it to make some kind of prediction in the future.\n",
    "\n",
    "Once our model instance is trained, you can use the predict() method to predict a target value given a set of features. In other words, use the model, along with some unlabelled data to predict the label.\n",
    "\n",
    "Note, data you predict on has to be in the same shape as data you trained on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
