{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QxfmwOzapfE"
   },
   "source": [
    "Convolutional Neural Networks (CNN) are used for classifying images and Computer Vision.\n",
    "\n",
    "To implement Neural Networks, we will use Tensorflow library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whrNq1W5a5zF"
   },
   "source": [
    "## Description of the dataset\n",
    "\n",
    "**Build a CNN to recognize whether the input image is cat or dog**\n",
    "\n",
    "1) Single Prediction - cat and dog image\n",
    "\n",
    "2) Training - 4000 images of cat and 4000 images of dog\n",
    "\n",
    "3) Test set - 1000 images of cat and 1000 images of dog\n",
    "\n",
    "Total 10,000 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2yF0s-_gQL5"
   },
   "source": [
    "**As the dataset is very big, we will not implement all on Google Collab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dNf7JO1LgvYC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwosvbwHARN7",
    "outputId": "9417c502-63d8-44e2-d0ee-82326cd124b2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "bWK3iHdugwDC",
    "outputId": "946503c3-5868-435f-f63d-5120fa00918e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkHiKj8zgevV"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf1z-tRwhM7-"
   },
   "source": [
    "### Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FxGnTZmFaqPq"
   },
   "outputs": [],
   "source": [
    "## Apply transformations on the images of the Training Set - to avoid overfitting\n",
    "\n",
    "# these transformations are geometrical like zoom in/out, rotate, flips - image augmentation\n",
    "# in this way our CNN wont be over trained on these images \n",
    "\n",
    "# Keras is a deep learning library which is integrated into Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7qPytmmhp4_",
    "outputId": "81235975-90fb-442f-e6a2-142e1894b56d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7999 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create train_datagen object \n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "# rescale - feature scaling applied on the pixels (absolutetly necessary for neural networks)\n",
    "\n",
    "# Import the training dataset \n",
    "training_set = train_datagen.flow_from_directory('image_dataset/training_set',  # change path of train set\n",
    "                                                 target_size = (64, 64), # final size of image to be fed into CNN\n",
    "                                                 batch_size = 32, # how many images in a batch to train at a time \n",
    "                                                 class_mode = 'binary') # binary or categorical (multiclass) classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUV7t6i2IEaG"
   },
   "source": [
    "### Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xORNGKFUB1Ad",
    "outputId": "e054f459-1df0-4924-d00e-1d49a8c2c3cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# New images - when deploying our model in production\n",
    "\n",
    "# we have to the images of the test set intact --> so NO TRANSFORMATION!\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255) # test set must also be feature scaled \n",
    "\n",
    "test_set = test_datagen.flow_from_directory('image_dataset/test_set', # change path of test set \n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSnjV9FJIc6Y"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrSrzGMYIkXD"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OXgJNqhZIHnb"
   },
   "outputs": [],
   "source": [
    "# Sequential library is a part of Keras library. Keras library is integrated into Tenserflow\n",
    "\n",
    "cnn = tf.keras.models.Sequential() # intialize the network, cnn object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIe5AxeotdyN"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5buqX4qOIHvE"
   },
   "outputs": [],
   "source": [
    "# Create the fully connected layer object. \n",
    "# using Convolutional class from Keras (Tensorflow) - tf.keras.Conv2D \n",
    "\n",
    "# filters --> no. of feature detectors to apply to our image\n",
    "# kernel_size --> size of the filter (rows and cols) e.g. size = 3 -> means 3x3\n",
    "# rectifier function - 'relu' (activation function)\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "\n",
    "# input_shape --> specifiy shape of inputs images (1st time)\n",
    "# coloured image --> input_shape=[X, X, 3]\n",
    "# B/W image --> input_shape=[X, X, 1]\n",
    "\n",
    "### 1) Feature Map Created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "363EGfhrtjrS"
   },
   "source": [
    "### Step 2 - Pooling (Max Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OB7JAYQ2IH1S"
   },
   "outputs": [],
   "source": [
    "# Apply MaxPool from Keras \n",
    "\n",
    "# pool_size --> 2 (which means 2x2)\n",
    "# strides -> shift pooling frame by 2 steps (2x2 pixels)\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "### 2) Pooled Feature Map Created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7kHFXl5tlIU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ugKoNBAHIH4I"
   },
   "outputs": [],
   "source": [
    "# Convolutional + Max Pooling for another layer\n",
    "\n",
    "# remove input_shape parameter (it exists only in 1st layer)\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')) \n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# we can add more layers. More layers is better, but it increases computational costs\n",
    "# also shape of layers must be same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OEaPMkEtos4"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XmRSWMT3IH6c"
   },
   "outputs": [],
   "source": [
    "# Create the Flattening layers (1D layers) by Flattern class from Keras\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VzD0a_EtpZc"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w72c4ioHIIB_"
   },
   "outputs": [],
   "source": [
    "# Now create a full connected layer by Dense class  \n",
    "\n",
    "# units --> no. of hidden neurons (a larger number is preferred for image classfication)\n",
    "# activation function --> rectifier function \n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PjTFnF5ttbx"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h-Nr-9jZtqF6"
   },
   "outputs": [],
   "source": [
    "# change units and activation function\n",
    "\n",
    "# Binary classification - 0 or 1, so dimension of neuron is 1 (1 output neuron)\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) # sigmoid activation function (gives probability)\n",
    "\n",
    "# for multiclass classification, output layer dimensions will depend on no.of classes\n",
    "\n",
    "# for binary classification - activation function is 'sigmoid'\n",
    "# for multiclass classification - activation function is 'softmax'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlUqKeSvt4gZ"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqgKGajj6Ruj"
   },
   "source": [
    "### Compiling the CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dqa2QMfjtqIk"
   },
   "outputs": [],
   "source": [
    "# using optimizer and loss function, with an evaluation metric\n",
    "\n",
    "# best optimizer - stochastic gradient descent (adam optimizer) [reduces loss error]\n",
    "# loss function, i.e. Cost function J theta - difference between prediction and real result\n",
    "\n",
    "# for binary classification - loss function is 'binary_crossentropy'\n",
    "# for multiclass classification - loss function is 'categorical_crossentropy'\n",
    "\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utcUVOXK6epv"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BUv6L3emtqNP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 94s 371ms/step - loss: 0.6739 - accuracy: 0.5783 - val_loss: 0.6239 - val_accuracy: 0.6645\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 85s 339ms/step - loss: 0.6172 - accuracy: 0.6596 - val_loss: 0.5722 - val_accuracy: 0.7050\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.5666 - accuracy: 0.7085 - val_loss: 0.5854 - val_accuracy: 0.6930\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 71s 283ms/step - loss: 0.5405 - accuracy: 0.7290 - val_loss: 0.5226 - val_accuracy: 0.7585\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 65s 261ms/step - loss: 0.5245 - accuracy: 0.7393 - val_loss: 0.5270 - val_accuracy: 0.7435\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 68s 270ms/step - loss: 0.4907 - accuracy: 0.7548 - val_loss: 0.5102 - val_accuracy: 0.7615\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.4768 - accuracy: 0.7710 - val_loss: 0.4841 - val_accuracy: 0.7765\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 72s 289ms/step - loss: 0.4607 - accuracy: 0.7830 - val_loss: 0.4526 - val_accuracy: 0.7895\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.4561 - accuracy: 0.7812 - val_loss: 0.4398 - val_accuracy: 0.7940\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 0.4348 - accuracy: 0.7905 - val_loss: 0.4383 - val_accuracy: 0.8005\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 77s 307ms/step - loss: 0.4236 - accuracy: 0.8061 - val_loss: 0.4442 - val_accuracy: 0.8025\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 84s 336ms/step - loss: 0.4052 - accuracy: 0.8090 - val_loss: 0.4599 - val_accuracy: 0.7905\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 88s 351ms/step - loss: 0.3974 - accuracy: 0.8166 - val_loss: 0.4677 - val_accuracy: 0.7900\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 87s 348ms/step - loss: 0.3808 - accuracy: 0.8271 - val_loss: 0.4416 - val_accuracy: 0.7970\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 63s 252ms/step - loss: 0.3700 - accuracy: 0.8324 - val_loss: 0.4372 - val_accuracy: 0.8010\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 0.3646 - accuracy: 0.8384 - val_loss: 0.4219 - val_accuracy: 0.8130\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.3544 - accuracy: 0.8412 - val_loss: 0.4612 - val_accuracy: 0.8030\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 80s 321ms/step - loss: 0.3400 - accuracy: 0.8519 - val_loss: 0.4424 - val_accuracy: 0.8170\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 69s 276ms/step - loss: 0.3203 - accuracy: 0.8589 - val_loss: 0.4579 - val_accuracy: 0.8170\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 69s 274ms/step - loss: 0.3108 - accuracy: 0.8655 - val_loss: 0.4605 - val_accuracy: 0.8105\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 118s 471ms/step - loss: 0.3086 - accuracy: 0.8690 - val_loss: 0.4649 - val_accuracy: 0.8180\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 55s 220ms/step - loss: 0.3011 - accuracy: 0.8717 - val_loss: 0.4525 - val_accuracy: 0.8190\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 105s 422ms/step - loss: 0.2791 - accuracy: 0.8797 - val_loss: 0.4933 - val_accuracy: 0.8130\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 58s 234ms/step - loss: 0.2816 - accuracy: 0.8794 - val_loss: 0.4535 - val_accuracy: 0.8095\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.2651 - accuracy: 0.8864 - val_loss: 0.4846 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c87fd93ca0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and Test at the same time\n",
    "\n",
    "# same fit method, but neural networks will have extra hyperparameters\n",
    "# no. of epocs --> no. of iterations the neural network is trained (improves accuracy over time) (make it small as we have 10,000 images)\n",
    "\n",
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, \n",
    "\n",
    "**Training Accuracy** = 0.8864\n",
    "\n",
    "**Test Accuracy** = 0.8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9is5KtZ6lJs"
   },
   "source": [
    "## Part 4 - Making a single prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GbaBVM98tqPZ"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image \n",
    "\n",
    "# load the images - give path of image\n",
    "# image size must be the same as the one used in training the model\n",
    "test_image = image.load_img('image_dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64)) \n",
    "\n",
    "# Conver the images from pixels to numpy array\n",
    "test_image = image.img_to_array(test_image) \n",
    "\n",
    "# CNN was not trained on single image, but on batch of images. This must also be specified for predict method\n",
    "# Place the image in a batch (extra dimension)\n",
    "test_image = np.expand_dims(test_image, axis = 0) # axis = 0 --> 1st dimension\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "# The way to figure out 0 or 1, is to call the class indices attritbute from our training set object\n",
    "training_set.class_indices\n",
    "\n",
    "# result also has batch dimension - result[0][0] - access batch, then single element within the batch\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AIpeCBIVtqRy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Convolutional Neural Networks (CNN).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
