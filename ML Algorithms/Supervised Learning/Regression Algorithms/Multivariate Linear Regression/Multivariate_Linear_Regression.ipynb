{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multivariate_Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUTHB9NjNvcA"
      },
      "source": [
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np # for matrix and vector computations\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV5RmwW4OKrR"
      },
      "source": [
        "implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
        "\n",
        "The file Data/ex1data2.txt contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wmv5xM8OD14"
      },
      "source": [
        "## 1) Load the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pudWQgftOE97"
      },
      "source": [
        "data = np.loadtxt('ex1data2.txt', delimiter=',')\n",
        "X = data[:,:-1]\n",
        "y = data[:,-1]\n",
        "\n",
        "m = y.size # number of training samples"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRYxtcxwQc8H",
        "outputId": "eda13fef-6a07-4e63-d206-5402945ae212"
      },
      "source": [
        "# print out some data points\n",
        "print('{:>8s}{:>8s}{:>10s}'.format('X1', 'X2', 'y'))\n",
        "print('-'*26)\n",
        "\n",
        "for i in range(10): # say first 10 points\n",
        "    print('{:8.0f}{:8.0f}{:10.0f}'.format(X[i, 0], X[i, 1], y[i]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      X1      X2         y\n",
            "--------------------------\n",
            "    2104       3    399900\n",
            "    1600       3    329900\n",
            "    2400       3    369000\n",
            "    1416       2    232000\n",
            "    3000       4    539900\n",
            "    1985       4    299900\n",
            "    1534       3    314900\n",
            "    1427       3    198999\n",
            "    1380       3    212000\n",
            "    1494       3    242500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXRTWQY5VD-c"
      },
      "source": [
        "## 2) Feature Normalization\n",
        "\n",
        "By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZStW5y9iXBS0"
      },
      "source": [
        "- Subtract the mean value of each feature from the dataset.\n",
        "- After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations.”\n",
        "\n",
        "The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within ±2 standard deviations of the mean); this is an alternative to taking the range of values (max-min). In `numpy`, you can use the `std` function to compute the standard deviation. \n",
        "\n",
        "For example, the quantity `X[:, 0]` contains all the values of $x_1$ (house sizes) in the training set, so `np.std(X[:, 0])` computes the standard deviation of the house sizes.\n",
        "\n",
        "At the time that the function `featureNormalize` is called, the extra column of 1’s corresponding to $x_0 = 1$ has not yet been added to $X$. \n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "**Implementation Note:** When normalizing the features, it is important\n",
        "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
        "from the model, we often want to predict the prices of houses we have not\n",
        "seen before. Given a new x value (living room area and number of bedrooms), we must first normalize x using the mean and standard deviation that we had previously computed from the training set.\n",
        "</div>\n",
        "<a id=\"featureNormalize\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pECa3DLjQiYt"
      },
      "source": [
        " \"\"\"\n",
        "    Normalizes the features in X. returns a normalized version of X where\n",
        "    the mean value of each feature is 0 and the standard deviation\n",
        "    is 1. This is often a good preprocessing step to do when working with\n",
        "    learning algorithms.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The dataset of shape (m x n) dimensions (imp*: x0=1 is not added during feature scaling)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    X_norm : array_like\n",
        "        The normalized dataset of shape (m x n).\n",
        "    \n",
        "    Instructions\n",
        "    ------------\n",
        "    First, for each feature dimension, compute the mean of the feature\n",
        "    and subtract it from the dataset, storing the mean value in mu. \n",
        "    Next, compute the  standard deviation of each feature and divide\n",
        "    each feature by it's standard deviation, storing the standard deviation \n",
        "    in sigma. \n",
        "    \n",
        "    Note that X is a matrix where each column is a feature and each row is\n",
        "    a sample. You need to perform the normalization separately for each feature. \n",
        "    \n",
        "    Hint\n",
        "    ----\n",
        "    You might find the 'np.mean' and 'np.std' functions useful.\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSLhR0cnYBP3"
      },
      "source": [
        "# X.shape[1] --> size=2 which means 2 features\n",
        "\n",
        "def featureNormalize(X):\n",
        "  # Set values\n",
        "  X_norm = X.copy() \n",
        "  mu = np.zeros(X.shape[1])\n",
        "  sigma = np.zeros(X.shape[1])\n",
        "\n",
        "  # to do feature scaling separtely for each feature, take each row of the features at a time \n",
        "  mu = np.mean(X,axis=0)\n",
        "  sigma = np.std(X,axis=0)\n",
        "  X_norm = (X - mu) / sigma\n",
        "\n",
        "  return X_norm, mu, sigma"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVxlOmKXaNMJ",
        "outputId": "e7239251-826e-4bbd-8284-71e72a8f24ad"
      },
      "source": [
        "X_norm, mu, sigma = featureNormalize(X)\n",
        "\n",
        "print(f\"Computed Mean: {mu}\")\n",
        "print(f\"Computed Standard deviation: {sigma}\")\n",
        "\n",
        "# we are getting 2 values of mean and sigma --> corresponding to 2 features of the dataset\n",
        "\n",
        "# we are also getting X_norm --> Feature scaled values of X"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computed Mean: [2000.68085106    3.17021277]\n",
            "Computed Standard deviation: [7.86202619e+02 7.52842809e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NXetvpEcDO8"
      },
      "source": [
        "## 3) Gradient Descent\n",
        "\n",
        "the hypothesis function and the batch gradient descent update\n",
        "rule remain unchanged, as it was for simple linear regression.\n",
        "\n",
        "You can use the `shape` property of `numpy` arrays to find out how many features are present in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzVJAcxcdO4P"
      },
      "source": [
        "### 3.1 Update Equations\n",
        "\n",
        "The objective of linear regression is to minimize the cost function $J(\\theta)$\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
        "\n",
        "where the hypothesis $h_\\theta(x)$ is given by the linear model\n",
        "$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 x_1$$\n",
        "\n",
        "Recall that the parameters of your model are the $\\theta_j$ values. These are\n",
        "the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to\n",
        "use the **batch gradient descent algorithm**. In batch gradient descent, each\n",
        "iteration performs the update\n",
        "\n",
        "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$$\n",
        "\n",
        "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\\theta$).\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "**Implementation Note:** We store each sample as a row in the the $X$ matrix in Python `numpy`. To take into account the intercept term ($\\theta_0$), we add an additional first column to $X$ and set it to all ones. This allows us to treat $\\theta_0$ as simply another 'feature'.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb4pN3XQbvbR"
      },
      "source": [
        "#### Add a column of ones to X. The numpy function concatenate() joins arrays along a given axis. \n",
        "\n",
        "# Add intercept term to X\n",
        "X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)\n",
        "# ones will have m rows and 1 column"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP1Sjc_3ezkl"
      },
      "source": [
        "<a id=\"section2\"></a>\n",
        "### 3.2 Computing the cost $J(\\theta)$\n",
        "\n",
        "As you perform gradient descent to minimize the cost function $J(\\theta)$, it is helpful to monitor the convergence by computing the cost. Implement a function to calculate $J(\\theta)$ so you can check the convergence of your gradient descent implementation. \n",
        "\n",
        "Remember that the variables $X$ and $y$ are not scalar values. $X$ is a matrix whose rows represent the samples from the training set (feature) and $y$ (label) is a vector whose each element represent the value at a given row of $X$.\n",
        "<a id=\"computeCost\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w3FttjTe0WN"
      },
      "source": [
        " \"\"\"\n",
        "    Compute cost for linear regression with multiple variables.\n",
        "    Computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The dataset of shape (m x n+1).\n",
        "    \n",
        "    y : array_like\n",
        "        A vector of shape (m, ) for the values at a given data point.\n",
        "    \n",
        "    theta : array_like\n",
        "        The linear regression parameters. A vector of shape (n+1, )\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    J : float\n",
        "        The value of the cost function. \n",
        "    \n",
        "    Instructions\n",
        "    ------------\n",
        "    Compute the cost of a particular choice of theta. You should set J to the cost.\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHC2BAFU1mW6"
      },
      "source": [
        "def computeCost(X,y,theta):\n",
        "  m = y.size # no. of training samples\n",
        "  J = 0\n",
        "\n",
        "  h = np.dot(X,theta) # X and theta are matrices\n",
        "  J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))\n",
        "\n",
        "  return J"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJBqwplj1s--"
      },
      "source": [
        "<a id=\"section3\"></a>\n",
        "### 3.3 Gradient descent\n",
        "\n",
        "Complete a function which Implements gradient descent. Update $\\theta$ with each iteration of the loop. \n",
        "\n",
        "As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the cost $J(\\theta)$ is parameterized by the vector $\\theta$, not $X$ and $y$. That is, we minimize the value of $J(\\theta)$ by changing the values of the vector $\\theta$, not by changing $X$ or $y$. \n",
        "\n",
        "A good way to verify that gradient descent is working correctly is to look at the value of $J(\\theta)$ and check that it is decreasing with each step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpqIfwyG1owC"
      },
      "source": [
        "    \"\"\"\n",
        "    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
        "    gradient steps with learning rate `alpha`.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The input dataset of shape (m x n+1).\n",
        "    \n",
        "    y : array_like\n",
        "        Value at given features. A vector of shape (m, ), i.e. (mx1) dimensions\n",
        "    \n",
        "    theta : array_like\n",
        "        Initial values for the linear regression parameters. \n",
        "        A vector of shape (n+1, ), i.e. (n+1)x1 dimensions\n",
        "    \n",
        "    alpha : float\n",
        "        The learning rate.\n",
        "    \n",
        "    num_iters : int\n",
        "        The number of iterations for gradient descent. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    theta : array_like\n",
        "        The learned linear regression parameters. A vector of shape (n+1, ). This is the optimal theta\n",
        "        for which J is minimum\n",
        "    \n",
        "    J_history : list\n",
        "        A python list for the values of the cost function after each iteration.\n",
        "    \n",
        "    Instructions\n",
        "    ------------\n",
        "    Peform a single gradient step on the parameter vector theta.\n",
        "\n",
        "    While debugging, it can be useful to print out the values of \n",
        "    the cost function (computeCost) and gradient here.\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCtiV_RM13Sp"
      },
      "source": [
        "def gradient_descent(X,y,theta,alpha,num_iters):\n",
        "  m = y.size # or y.shape[0] # number of training samples\n",
        "\n",
        "  # make a copy of theta, to avoid changing the original array, since numpy arrays are passed by reference to functions\n",
        "  theta = theta.copy()\n",
        "\n",
        "  J_history = [] # Use a python list to store cost in every iteration\n",
        "\n",
        "  for i in range(num_iters):\n",
        "    theta = theta - (alpha/m) * (np.dot(X,theta) - y).dot(X)\n",
        "    # print(theta)\n",
        "\n",
        "    # save the cost J in every iteration\n",
        "    min_cost = computeCost(X,y,theta)\n",
        "    J_history.append(min_cost)\n",
        "    # print(J_history[i])\n",
        "\n",
        "  return theta, J_history # theta will return values equivalent to the no. of features available"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfUxV4-9_fyA",
        "outputId": "5488ac50-d441-40d0-f24c-20a6d0ab5555"
      },
      "source": [
        "# randomly initialize fitting parameters\n",
        "theta = np.zeros(3) # 3 for 3 features\n",
        "\n",
        "# some gradient descent settings\n",
        "iterations = 400\n",
        "alpha = 0.1\n",
        "\n",
        "theta, J_history = gradient_descent(X,y,theta,alpha,iterations)\n",
        "\n",
        "# Display the gradient descent's result\n",
        "print('theta computed from gradient descent: {:s}'.format(str(theta)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "theta computed from gradient descent: [340412.65957447 109447.79558639  -6578.3539709 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LepqnPF5-U70"
      },
      "source": [
        "### 3.4) Predict values \n",
        "\n",
        "predict the price of a 1650 sq-ft, 3 bedroom house.\n",
        "\n",
        "**At prediction, make sure you do the same feature normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwH7df_W2TjR",
        "outputId": "34f7a213-20ad-4503-ab94-633b2c4c592f"
      },
      "source": [
        "# Recall that the first column of X (column 0) is all-ones(x0=1)\n",
        "# Thus, it does not need to be normalized.\n",
        "\n",
        "X_array = [1,1650,3]\n",
        "X_array[1:3] = (X_array[1:3] - mu)/sigma\n",
        "# X_array[1:3] contains features after feature scaling\n",
        "\n",
        "# Predict\n",
        "price = np.dot(X_array,theta) # theta here is the optimal theta\n",
        "\n",
        "print(f'Predicted price of a 1650 sq-ft, 3 bedroom house (using gradient descent): ${price:.0f} ')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted price of a 1650 sq-ft, 3 bedroom house (using gradient descent): $293081 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IgqN_4SARhe"
      },
      "source": [
        "## 4) Selecting Learning Rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UlbV7o0Aa5c"
      },
      "source": [
        "Try out different learning rates for the dataset and find a learning rate that converges quickly. \n",
        "\n",
        "run gradient descent for about 50 iterations at the chosen learning rate. The function should also return the history of $J(\\theta)$ values in a vector $J$.\n",
        "\n",
        "After the last iteration, plot the J values against the number of the iterations.\n",
        "\n",
        "\n",
        "If your graph looks very different, especially if your value of $J(\\theta)$ increases or even blows up, adjust your learning rate and try again. We recommend trying values of the learning rate $\\alpha$ on a log-scale, at multiplicative steps of about 3 times the previous value (i.e., 0.3, 0.1, 0.03, 0.01 and so on). You may also want to adjust the number of iterations you are running if that will help you see the overall trend in the curve.\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "**Implementation Note:** If your learning rate is too large, $J(\\theta)$ can diverge and ‘blow up’, resulting in values which are too large for computer calculations. In these situations, `numpy` will tend to return\n",
        "NaNs. NaN stands for ‘not a number’ and is often caused by undefined operations that involve −∞ and +∞.\n",
        "</div>\n",
        "\n",
        "\n",
        "Notice the changes in the convergence curves as the learning rate changes. With a small learning rate, you should find that gradient descent takes a very long time to converge to the optimal value. Conversely, with a large learning rate, gradient descent might not converge or might even diverge!\n",
        "\n",
        "Using the best learning rate that you found, run gradient descent until convergence to find the final values of $\\theta$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "ThmtTrv6_SKu",
        "outputId": "414b60e3-95f5-440f-a23e-6284d31be6b7"
      },
      "source": [
        "# Plot the convergence graph\n",
        "plt.plot(np.arange(len(J_history)), J_history, lw=2)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost J(theta)')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Cost J(theta)')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYinCynDBB4G"
      },
      "source": [
        "## 5) Normal Equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7H5PyjVmzib"
      },
      "source": [
        "the closed-form solution to linear regression is\n",
        "\n",
        "$$ \\theta = \\left( X^T X\\right)^{-1} X^T\\vec{y}$$\n",
        "\n",
        "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no “loop until convergence” like in gradient descent. \n",
        "\n",
        "Remember that while you do not need to scale your features, we still need to add a column of 1’s to the $X$ matrix to have an intercept term ($\\theta_0$). The code in the next cell will add the column of 1’s to X for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaXtQftQA_27"
      },
      "source": [
        "# Load a new set of data\n",
        "data = np.loadtxt('ex1data2.txt', delimiter=',')\n",
        "X = data[:,:-1]\n",
        "y = data[:,-1]\n",
        "\n",
        "m = y.size # no. of training sample\n",
        "\n",
        "# Add x0=1 -> add a column of 1s to X matrix\n",
        "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0iuIIcen0fX"
      },
      "source": [
        " \"\"\"\n",
        "    Computes the closed-form solution to linear regression using the normal equation.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The dataset of shape (m x n+1).\n",
        "    \n",
        "    y : array_like\n",
        "        The value at each data point. A vector of shape (m, ).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    theta : array_like\n",
        "        Estimated linear regression parameters. A vector of shape (n+1, ).\n",
        "    \n",
        "    \n",
        "    function `np.linalg.pinv` for computing matrix inverse.\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KMgDZnBoBVn"
      },
      "source": [
        "# X.shape[1] --> 3 features\n",
        "\n",
        "def normalEqn(X,y):\n",
        "  theta = np.zeros(X.shape[1])\n",
        "\n",
        "  theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)\n",
        "\n",
        "  return theta # theta will return 3 values of optimal theta0,theta1,theta2 --> corresponding to 3 features"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe4TWwTsoJzG",
        "outputId": "1e5b1acd-85d8-4d69-b475-1ae2ffeec475"
      },
      "source": [
        "# Calculate the parameters from the normal equation\n",
        "theta = normalEqn(X,y)\n",
        "\n",
        "# Display normal equation's result\n",
        "print('Theta computed from the normal equations: {:s}'.format(str(theta)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theta computed from the normal equations: [89597.9095428    139.21067402 -8738.01911233]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ7P6fgJoL_n",
        "outputId": "d2e241db-c2ef-4fe1-9b9c-2224b186b960"
      },
      "source": [
        "# Prediction - Estimate the price of a 1650 sq-ft, 3 br house\n",
        "\n",
        "X_array = [1,1650,3]\n",
        "X_array[1:3] = (X_array[1:3] - mu)/sigma\n",
        "# X_array[1:3] contains features after feature scaling\n",
        "\n",
        "# Predict\n",
        "price = np.dot(X_array,theta) # theta here is the optimal theta\n",
        "\n",
        "print(f'Predicted price of a 1650 sq-ft, 3 bedroom house (using gradient descent): ${price:.0f} ')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted price of a 1650 sq-ft, 3 bedroom house (using gradient descent): $91511 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1OXVlfupEjP"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8jf0TZlpKrl"
      },
      "source": [
        "From gradient descent, prediction is $293,081\n",
        "\n",
        "From normal equation, prediction is $91,511\n",
        "\n",
        "* So which is correct?\n",
        "\n",
        "Both can be used for linear regression. \n",
        "However, size of the dataset in this case was less, so Normal Equation will give a correct answer\n",
        "\n",
        "If size of the dataset is more, say 10,000 rows, then Gradient Descent will give a correct answer (Normal equations don't work with huge number of data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTvHxDExo6kq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}